# bike-sharing-classification
Project to do classification using maching learning on Bike sharing dataset. Through multiple methods and multiple models, a decent results are obtained. The code can be find in [bike_sharing.ipynb](https://github.com/ricocahyadi777/bike-sharing-classification/blob/7a44b550bef482edd0e53b4d9b7708013f29b650/bike_sharing.ipynb)

## Overview
Bike sharing systems are the new business model of bike rentals where the whole process of membership, rental and return has become automatic. Through these systems, users can easily rent a bike from one station and return it at another station. Currently, there are over 500 bike-sharing programs around the world which is composed of over 500 thousand bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. <br/>
Apart from interesting real-world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for business and research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.

Dataset and more info can be found on https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset

Attribute Information:
bike_sharing.csv has the following fields 
- instant: record index
- dteday: date
- season: season (1: spring, 2: summer, 3: fall, 4: winter)
- yr: year (0: 2011, 1:2012)
- mnth: month ( 1 to 12)
- hr: hour (0 to 23)
- holiday: whether the day is holiday or not
- weekday: day of the week
- workingday: 1 if the day is neither a weekend nor a holiday, otherwise 0.
- weathersit: 
  + 1: Clear, Few clouds, Partly cloudy, Partly cloudy
  + 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
  + 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
  + 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
- temp: Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)
- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_maxt_min), t_min=-16, t_max=+50 (only in hourly scale)
- hum: Normalized humidity. The values are divided to 100 (max)
- windspeed: Normalized wind speed. The values are divided to 67 (max)
- casual: count of casual users
- registered: count of registered users
- cnt: count of total rental bikes including both casual and registered

## Approach
### Regularized Linear Regression
We wanted to build a regression model for column casual from at most 5 explanatory variables. This would requires some trail & error experimentation

We will use column casual as our response variable y. As we will have several ways to organize our explanatory variables, to better compare these models, let's fix the instances for training and test sets
 ```python
train = np.random.choice([True, False], num_row, replace = True, p = [0.5, 0.5])
y = bikes['casual'].values
y_train, y_test = y[train], y[~train]
 ```
 
First, we need to construct a very simple regression model from numerical columns as listed above. <br/>
In order to compare the coefficients to identify important columns, it is necessary to scale all columns to interval [0, 1] using MinMaxScaler.<br/>
We split the data into training and test data, then perform fit transformation on training data while transform on test data. <br/>
WHY? <br/>
If we will use the fit method on our test data too, we will compute a new mean and variance that is a new scale for each feature and will let our model learn about our test data too. Thus, what we want to keep as a surprise is no longer unknown to our model and we will not get a good estimate of how our model is performing on the test (unseen) data which is the ultimate goal of building a model using machine learning algorithm.<br/>
Reasoning cited from: https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe
```python
min_max_scaler = preprocessing.MinMaxScaler()
x1_train, x1_test = x1[train,:], x1[~train,:]
x1_train = min_max_scaler.fit_transform(x1_train)
x1_test = min_max_scaler.transform(x1_test)
```

Then let's try a simple regression 
```python
lasso = linear_model.Lasso(alpha = 1.0)
lasso.fit(x1_train, y_train)
print("{:12}: {}".format('Alpha:', 1.0))
print("{:12}: {}".format('R2 score:', lasso.score(x1_test, y_test)))
print("Variables")
for i in range(len(selected_cols)):
    print("{:12}: {}".format(selected_cols[i], lasso.coef_[i]))
```
Resulting in:<br/>
![image](https://user-images.githubusercontent.com/63791918/237012336-b4d86a47-09c1-4102-afe4-f04652ce9657.png)

The $R^2$ score is 0.42, and there are already 5 columns used for this model (columns with non-zero coefficients).
Let's now decrease alpha to see if we can obtain a model with $R^2$ above 0.5.
```python
for alpha in [1.0, 0.5, 0.2, 0.1]:
    lasso = linear_model.Lasso(alpha = alpha)
    lasso.fit(x1_train, y_train)
    print("{:12}: {}".format('Alpha:', alpha))
    print("{:12}: {}".format('R2 score:', lasso.score(x1_test, y_test)))
    for i in range(len(selected_cols)):
        print("{:12}: {}".format(selected_cols[i], lasso.coef_[i]))
```
After trying multiple alpha, the best we get is: <br/>
![image](https://user-images.githubusercontent.com/63791918/237013884-7fc59dfb-410b-4123-a6c5-7bf8a8061161.png)

Unfortunately, is still doesn't reach 0.5. <br/>
Now let's consider the column 'weekday'. Although it is a numerical column, the numbers does not have a quantitative meaning, they are just indicators. <br/>
Such columns are called Nominal columns, i.e., for each value x in the column 'weekday', we create a column called 'weekday=x', a record will get a value of 1 on this column if its orginal value is x, otherwise 0. <br/>
Pandas conveniently provides a function 'get_dummies' for this purpose.<br/>
Both columns 'season' and 'weekday' are nominal columns.<br/>
```python
seasons = pd.get_dummies(bikes['season'], prefix = 'season_=')
weekdays = pd.get_dummies(bikes['weekday'], prefix = 'weekday_=')

bikes = pd.concat([bikes, seasons, weekdays], axis = 1)
```
Another interesting column is column 'weathersit'. The description tells us these four situations are very different.
1. Clear, Few clouds, Partly cloudy, Partly cloudy
2. Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
3. Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
4. Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog

First, Situation 1 is mutually exclusive from the rest. Second, there is a degree of severity associated from situation 2 to 4, meaning if situation 3 happens, the weather has gone passed situation 2. This is so-called ordinal columns.
For ordinal columns, we should turn on lower grade indicator automatically if the value is higher. For example, indicator for weathersit2 should be 1 all records with weathersit 2, 3, 4.
Third, there are very few records for weathersit = 4, that we do not need to create one column just for weathersit 4.

```python
bikes['weathersit_=_1'] = 1 * (bikes['weathersit'] == 1)
bikes['weathersit_>=_2'] = 1 * (bikes['weathersit'] >= 2)
bikes['weathersit_>=_3'] = 1 * (bikes['weathersit'] >= 3)
```
Then, we run another regression. Resulting in: <br/>
![image](https://user-images.githubusercontent.com/63791918/237015080-62f4b28f-e22f-4403-b9d3-2c28231523a3.png)

However, these indicator columns are not as significant as columns, 'yr', 'hr', 'workingday', 'temp' and 'hum'. Even with these five columns only, the performace is about the same.

![image](https://user-images.githubusercontent.com/63791918/237015532-f3c4d8af-e8d9-4e58-9883-cd3e146f1315.png)

There is a better way to use column 'dteday' since dates are not numbers. We first convert it to numbers using data structure datetime with functions strptime and strftime. <br/>
Then we create a column The for loop below calculates the number of days since 2011-1-1 with hours being the fractional part, so it is continuous from 2011-12-31 to 2012-1-1.

```python
days = np.zeros((num_row))
for i in range(num_row):
    dtime = bikes.iloc[i]['dteday']
    dtime = datetime.strptime(dtime, '%Y-%m-%d')
    days[i] = int(datetime.strftime(dtime, '%j')) - 1 + bikes.iloc[i]['hr'] / 24 + 365 * bikes.iloc[i]['yr']
```

The new regression resulting in:<br/>
![image](https://user-images.githubusercontent.com/63791918/237016389-9cef66c2-9408-42d8-ba0d-86ae4e353974.png)

Still not much result.<br/>
Maybe we can use the 'hr' variable. <br/>
The variable hr is a cyclic variable, meaning value 23 is next to 0 and 22, however, if we treat it like ordinary numerical column, 23 is very far from 0 as compared to 22, which may result in wrong modeling in this column. <br/>
With the business domain knowledge on bike sharing, there must be peak hours and non-peak hours for bike usage. <br/>
One possible way to better model variable hr is to calculate the difference of the current hour to the peak hour. <br/>
Notice the differences_in_hr variable. The calculation is not as straightforward due to the cyclic nature of hour.<br/>
After calculation we simply add the column to the dataframe before doing the next regression.
```python
hr_diff = np.zeros((num_row), dtype = int)
hour_info = [0 for i in range(24)]
for i in range(num_row):
    hour_info[bikes.iloc[i]['hr']] = hour_info[bikes.iloc[i]['hr']] + bikes.iloc[i]['casual']
    
peak_hr = hour_info.index(max(hour_info))
print("Peak hour : {}".format(peak_hr))
for i in range(num_row): 
    # calculate the difference in hours from the peak hour
    differences_in_hr = min(abs((bikes.iloc[i]['hr']- peak_hr)), abs((bikes.iloc[i]['hr']+24 - peak_hr)))
    hr_diff[i] = differences_in_hr

df = pd.DataFrame(hr_diff, columns = ['hr_diff'])
bikes = pd.concat((bikes, df), axis =1)
```

Resulting in the desired result:<br/>
![image](https://user-images.githubusercontent.com/63791918/237017680-b293abb6-642f-4dca-aaa9-754d223d7500.png)

### Multi-class Classification using Logistic Regression
Using the same dataset, however to make it a classification problem, we are going to discretize the column casual. 
From the data, we can get 3 segments of data almost equal in size by classifying it as follows:
+ 0 to 6 to low usage
+ 7 to 33 to medium usage 
+ 34 onward to high usage
```python
y = bikes['casual'].values
y = np.array([1 if i < 7 else 2 if i < 34 else 3 for i in y])
```
We want to visualize overfitting happens when the regularization parameter C is large. So we select two relatively smaller data sets for training and test as follows.
```python
np.random.seed(2023)
selection = np.random.choice(['train', 'test', 'rest'], num_row, replace = True, p = [0.01, 0.09, 0.9])
y_train, y_test = y[selection == 'train'], y[selection == 'test']
```
Then we use a 'OneVsRest' Logistic Regression Classifier.
```python
selected_cols = ['yr', 'mnth', 'hr', 'holiday', 'workingday', 'temp', 'atemp', 'hum', 'windspeed']
x = bikes[selected_cols].values
x_train, x_test = x[selection == 'train',:], x[selection == 'test',:]

multi_ovr_estimator = linear_model.LogisticRegression(solver='lbfgs',multi_class='ovr')
multi_ovr_estimator.fit(x_train, y_train)
y_ovr_predict = multi_ovr_estimator.predict(x_test)
```

Finally we visualize the results. <br/>
![image](https://github.com/ricocahyadi777/bike-sharing-classification/assets/63791918/078e636c-2fad-4cf7-8185-cb55192f526f)

![image](https://github.com/ricocahyadi777/bike-sharing-classification/assets/63791918/ba04c29e-c46a-46d0-a4d5-d90dabb0ecaa)

Finally we are to visualize the change of training error and test error in response to the change of complexity trade-off parameter C. <br/>
Note that logistic regression actually maximizes the log likelihood, with the convention that cost function is usually a minimization function.
```python
num_C = 10
C = [1.0] * num_C
for i in range(num_C):
    C[i] = pow(10, i-3)
logit = [None] * num_C
inv_log_likelihood_train = [0.0] * num_C
inv_log_likelihood_test = [0.0] * num_C

for i in range(num_C):
    logreg_multi = LogisticRegression(C=C[i], solver='lbfgs', multi_class='multinomial')
    logreg_multi.fit(x_train, y_train)
    print('Accuracy of logistic regression classifier with C:{} on test set: {:.10f}'.format(C[i],logreg_multi.score(x_test, y_test)))
    
    y_train_predict = logreg_multi.predict(x_train)
    y_train_predict_proba = logreg_multi.predict_proba(x_train)
    
    y_test_predict = logreg_multi.predict(x_test)
    y_test_predict_proba = logreg_multi.predict_proba(x_test)
    
    inv_log_likelihood_train[i] = log_loss(y_train, y_train_predict_proba) 
    inv_log_likelihood_test[i] = log_loss(y_test, y_test_predict_proba)
```

Finally we visualize the result: <br/>
![image](https://github.com/ricocahyadi777/bike-sharing-classification/assets/63791918/e1c67666-6257-481c-8797-2fcb1c2b6555)
![image](https://github.com/ricocahyadi777/bike-sharing-classification/assets/63791918/aa0eb7dd-0a2a-4145-8140-ee0c2dcc0541)
